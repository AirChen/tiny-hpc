物理

流处理器 （SP 或核心）。SP 是 GPU 上的主要处理单元，能够同时对多个数据元素执行计算。您可以将 SP 视为单个学生（1 名学生为 1 个 SP）。SP （学生）越多，可以同时处理的任务数量就越大。
流式多处理器 （SM 或多处理器）是 SP 的集合或分组。可以将其理解为容纳多个 SP 的班级或教室。SM 充当更高级别的单元，负责管理和协调其内部 SP 之间的任务执行。

逻辑

逻辑对应线程、块、网格
线程可以理解为一个工作单元，每个线程代表一个待执行的独立任务或作业。因此，一个线程对应一个任务。
块是指一组线程的集合或组。它表示一批或一组一起执行的相关任务。但是，一个块中的线程数量有上限，通常为 1024 个线程。此限制由计算机架构决定，并且适用于大多数 GPU。
总而言之，线程代表单个任务或作业，而块是一组共同执行一组相关任务的线程。
网格 (Grid) 指的是一组块的集合或集合。它代表一个包含多个块的更高级别的分组。每个块由多个线程组成，多个块共同构成一个网格。
重点理解线程和块非常重要，因为它们是并行执行的基本单元。

物理和逻辑交互
Warp 是指一组并行执行的线程。在大多数 GPU 架构中，一个 Warp 通常包含 32 个线程。GPU 以 SIMD（单指令多数据）方式处理指令，即一条指令在一个 Warp 内同时对多个数据元素执行。这意味着 Warp 内的所有 32 个线程都执行同一条指令，但操作不同的数据。
32 这个数字意义重大，因为它代表了大多数 GPU 架构中 Warp 的大小。它决定了并行执行单元中可以同时处理多少个线程。通过并行执行多个 Warp，GPU 可以实现高吞吐量并高效利用其处理资源。
虽然我们用 Warp 来类比小组组长，但它们并不计入课堂内的个人成员。例如，如果一个教室有 50 名学生和 5 名组长，那么 SP（学生）的数量仍然是 50，而不是 55。


内存
使用 CUDA 进行编程的一大优势在于，我们可以自由选择要使用的内存 （这意味着在初始化值或变量时，我们可以指定将其存储在特定的内存位置），而不是让计算机决定使用哪个内存。得益于此功能，我们可以充分利用不同类型的内存来优化程序。
当我们谈论内存时 ，我们通常将其分为两种主要类型： 物理内存和逻辑内存 。
物理内存： 指计算机中实际的硬件内存。它包括 RAM 模块和硬盘（HDD/SSD）等存储设备。物理内存是直接存储数据和程序的地方，可供处理器快速访问。
逻辑内存（虚拟内存）： 这是操作系统和程序可以访问的地址空间。逻辑内存不一定与物理内存一一对应。操作系统通常管理逻辑地址和物理地址之间的映射。这种管理有助于为系统上运行的程序分配和管理内存。

这里我们有一个熟悉的概念，称为范围（scope），它在理解如何在 GPU 的逻辑内存中分配和管理线程和块等资源方面起着至关重要的作用。
本地内存： 每个线程都可以使用自己的本地内存 ，用于存储临时变量。本地内存的（scope）最小 ，并且每个线程都拥有自己的专属内存。
共享内存：同一块内的线程可以通过共享内存共享数据。与访问全局内存相比，这使得同一块内的线程能够更快地通信和访问数据。
全局内存： 这是 GPU 中最大的内存 ， 所有块中的所有线程都可以访问。但是，访问全局内存通常比其他类型的内存慢，因此需要进行优化以避免性能下降。
纹理内存和常量内存： 这些是 GPU 中的特殊内存类型， 经过优化，可用于访问特定数据类型，例如纹理或常量值。所有块中的所有线程都可以访问这些内存类型。

每个 SM 都拥有自己专用的共享内存、缓存、常量内存和寄存器内存 。但是， 多个 SM 共享相同的全局内存 。
在这种安排下，SM 可以高效地管理各自的本地资源，例如用于块内通信的共享内存，并且每个 SM 的处理单元 (SP) 可以独立地执行其分配的任务。全局内存的共享允许不同 SM 之间进行协调和数据交换，从而使它们能够协同处理更大规模的计算任务。


正如我之前提到的， CPU（主机） 和 GPU（设备） 是两个独立的组件，各自拥有独立的内存，它们之间无法直接访问。数据必须通过 PCIe（外围组件互连高速总线）——一种常见的接口—— 来回复制。https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/
全局内存（又称设备内存） 是 GPU 内部最大的内存 ，由于其大小， 访问速度相对较慢 ，访问延迟仅次于 PCIe。全局内存的主要目的是存储大量数据。

共享内存和缓存是访问速度较快的内存类型，但与全局内存相比容量较小 。
由于共享内存和缓存内存访问速度快，我们经常在计算过程中使用它们来存储数据。典型的方法是先将所有数据从 CPU 复制到 GPU，并将其存储在全局内存中。然后，我们将数据分解成更小的部分（块），并将它们推送到共享内存中进行计算。计算完成后，结果将被推送回全局内存。

前面提到过， Texture Memory 和 Constant Memory 是 GPU 中特殊的内存类型， 专门针对访问特定数据类型（例如图像（纹理）或常量值） 进行了优化。这两种内存类型的访问速度相当快，堪比共享内存。
因此，使用纹理内存和常量内存的目的是优化数据访问，并减少共享内存的计算负载。我们不必将所有数据都推送到共享内存中，而是将部分数据分配给纹理内存和常量内存。这种分配策略能够利用纹理内存和常量内存的优化访问能力，从而提升内存性能。

对于每个线程来说，本地内存和寄存器文件是两种不同的内存类型。寄存器文件是每个线程可用的最快内存。当内核变量无法放入寄存器文件时，它们将使用本地内存。

同步和异步
同一块内的线程会同步，但仅限于在同一个作业中转到下一个任务时；它们仍然是异步的（类似于我示例中学生的作业解答速度）。块（或不同块中的线程）之间是异步的。

Unified memory
统一内存是一种特殊类型的内存（位于 CPU 上），CPU 和 GPU 都可以直接访问，而无需在两种不同的内存类型之间来回复制数据。这就是为什么统一内存是基于零拷贝原则来调用的。
虚拟内存（开发人员视图）：从这个角度来看，统一内存是 CPU 和 GPU 之间的统一内存（CPU 和 GPU 都可以直接访问它）。
物理内存（计算机视角）：正如我之前提到的，CPU 和 GPU 拥有独立的内存，并且无法直接访问（只能通过 PCI）。这里，统一内存位于 CPU 上，但由于零拷贝机制，我们将统一内存视为 CPU 和 GPU 的统一内存。
零拷贝：这是一种数据传输优化方法，数据会自动从一个设备（例如 CPU）的内存直接传输到另一个设备（例如 GPU），而无需经过中间步骤（例如缓冲区）。这显著减少了数据复制所需的时间和资源，从而提高了性能。
需要注意的是，页面错误仅在使用零拷贝机制（尤其是统一内存）时才会发生。像 cudaMemcpy 这样的传统方法不会出现页面错误，因为在这些情况下，我们指定在处理之前应完全复制数据，类似于遵循逐步顺序。

NVIDIA 开发了一项功能，允许我们从一开始就指定将数据存储在 pinned mem 中（仅需要从主机到设备进行一次复制），而无需进行 2 次复制。

对于 CUDA-C 代码，我们需要关注两个概念：计算受限 (Compute Bound) 和内存受限 (Memory Bound)。这两个概念可以简单理解为两个问题：在计算上花费过多时间，还是在内存加载/存储操作上花费过多时间。Streaming 技术可以帮助我们解决内存受限的问题。
性能工具使用 https://github.com/CisMine/Guide-NVIDIA-Tools
大量的数据将被分割和处理。得益于流分支机制，分割和处理将并行进行，而不是顺序进行。
Asynchronous version 1: loop over {copy, kernel, copy}
Asynchronous version 2: loop over copy, loop over kernel, loop over copy

数据竞争 总之，只要记住：在 CUDA 中编码时，要注意多个线程访问同一个值进行处理的现象。

And here are some atomic functions:
以下是一些原子函数：

atomicAdd(result, array_add[tid]); – adds array_add[tid] to result.
atomicSub(result, array_sub[tid]); – subtracts array_sub[tid] from result.
atomicMax(result, array_max[tid]); – computes the maximum of result and array_max[tid].
atomicMin(result, array_min[tid]); – computes the minimum of result and array_min[tid].
Additionally, there are other interesting atomic functions like atomicCAS (Compare And Swap) and atomicExch (Exchange).

共享内存是 GPU 中速度最快的内存（仅次于寄存器文件），访问共享内存的范围是同一块内的线程。
每当将数据从全局复制到共享内存时，我们必须使用 __syncthreads() 来同步同一块内的线程，以避免出现竞争条件 。这是因为虽然块中的线程在逻辑上并行运行，但并非所有线程都能在物理上同时执行。